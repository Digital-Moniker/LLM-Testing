{"cells":[{"cell_type":"markdown","metadata":{"id":"2wHkktWPSNTH"},"source":["## Querying Groq's LLM API vs Open AI LLM API:"]},{"cell_type":"markdown","metadata":{"id":"r7pQ_RnadbLz"},"source":["[![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LinkedInLearning/generative-ai-and-llmops-deploying-and-managing-llms-in-production-4465782/blob/main/ch-02/challenge_API_limitations.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0-Q5FXjfKHK"},"outputs":[],"source":["!pip install Groq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9esvOP1ap8R","executionInfo":{"status":"ok","timestamp":1727423654218,"user_tz":-60,"elapsed":3056,"user":{"displayName":"","userId":""}},"outputId":"dd436670-85f6-4758-c431-efc388f2a22b","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","output_type":"stream","text":["··········\n"]}],"source":["from groq import Groq\n","import getpass\n","\n","# TODO: Create a Groq Client\n","# You can get token from console.groq.com\n","client = Groq(api_key=getpass.getpass())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMXH-N19gW23","executionInfo":{"status":"ok","timestamp":1727424673466,"user_tz":-60,"elapsed":396,"user":{"displayName":"","userId":""}},"outputId":"dec284da-be3c-48f5-f4a8-381b5ab914e0","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: llama3-8b-8192\n","Total tokens: 1200\n","Total time: 3.5 seconds\n","Throughput: 342.86 tokens/second\n","-----------\n","Model: llama3-70b-8192\n","Total tokens: 1200\n","Total time: 3.5 seconds\n","Throughput: 342.86 tokens/second\n","-----------\n","Model: mixtral-8x7b-32768\n","Total tokens: 1200\n","Total time: 3.5 seconds\n","Throughput: 342.86 tokens/second\n","-----------\n","Model: gemma-7b-it\n","Total tokens: 1200\n","Total time: 3.5 seconds\n","Throughput: 342.86 tokens/second\n","-----------\n"]}],"source":["# List of Grok models to evaluate\n","model_list = ['llama3-8b-8192', 'llama3-70b-8192', 'mixtral-8x7b-32768', 'gemma-7b-it']\n","prompt = \"Write a blog about taking Generative AI applications to production\"\n","\n","# Placeholder function to simulate interaction with Grok API\n","# Replace this with actual API call to the Grok models\n","def generate_text(model, prompt):\n","    # Simulated response for illustration purposes\n","    response = {\n","        \"usage\": {\n","            \"total_tokens\": 500,  # Example token count for the model's output\n","            \"completion_time\": 3.5  # Example time in seconds for model's completion\n","        }\n","    }\n","    return response\n","\n","# Iterate over each model in the list and evaluate performance\n","for model in model_list:\n","    chat_completion = generate_text(model, prompt)\n","\n","    # Extracting values from the chat_completion response\n","    total_tokens = chat_completion[\"usage\"][\"total_tokens\"]\n","    total_time = chat_completion[\"usage\"][\"completion_time\"]\n","\n","    # Calculating throughput (tokens per second)\n","    throughput = total_tokens / total_time if total_time > 0 else 0\n","\n","    # Printing model performance and latency metrics\n","    print(f\"Model: {model}\")\n","    print(f\"Total tokens: {total_tokens}\")\n","    print(f\"Total time: {total_time} seconds\")\n","    print(f\"Throughput: {throughput:.2f} tokens/second\")\n","    print(\"-----------\")"]},{"cell_type":"markdown","source":[],"metadata":{"id":"BoKZq1zriPD3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nUGTqYWtgdI_","executionInfo":{"status":"ok","timestamp":1727423804891,"user_tz":-60,"elapsed":6174,"user":{"displayName":"","userId":""}},"outputId":"ede442f6-0f72-423f-fd1d-40d7c368b362","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: llama3-8b-8192\n","Total tokens: 784\n","Total time: 0.635833333\n","Throughput: 1233.02752358219\n","-----------\n","Model: llama3-70b-8192\n","Total tokens: 777\n","Total time: 2.343459721\n","Throughput: 331.5610646247587\n","-----------\n","Model: mixtral-8x7b-32768\n","Total tokens: 815\n","Total time: 1.293559627\n","Throughput: 630.0444007286569\n","-----------\n","Model: gemma-7b-it\n","Total tokens: 524\n","Total time: 0.617093231\n","Throughput: 849.1423559303311\n","-----------\n"]}],"source":["model_list=['llama3-8b-8192', 'llama3-70b-8192', 'mixtral-8x7b-32768', 'gemma-7b-it']\n","prompt=\"Write a blog about taking Generative AI applications to production\"\n","\n","for model in model_list:\n","    chat_completion = generate_text(model, prompt)\n","    total_tokens=chat_completion.usage.total_tokens\n","    total_time=chat_completion.usage.completion_time\n","    print(f\"Model: {model}\")\n","    print(f\"Total tokens: {total_tokens}\")\n","    print(f\"Total time: {total_time}\")\n","    print(f\"Throughput: {total_tokens/total_time}\")\n","    print(\"-----------\")"]},{"cell_type":"markdown","source":["llama3-8b-8192: Exceptional throughput and quick response time enhance user experience, but higher token usage may increase costs for large-scale deployments.\n","\n","llama3-70b-8192: Provides detailed responses with a larger model, but slower response time and slightly higher token usage impact real-time usability and cost efficiency.\n","\n","mixtral-8x7b-32768: Balanced performance with good throughput and moderate response time, offering a good user experience for longer inputs, though cost may increase with high token usage.\n","\n","gemma-7b-it: Fast and responsive with lower token usage, ensuring a cost-efficient option for applications where quick interactions and moderate output are prioritized."],"metadata":{"id":"Tk8r5w5gfNQ3"}},{"cell_type":"markdown","metadata":{"id":"2hLOlOqGka_U"},"source":["## OpenAI"]},{"cell_type":"code","source":["pip install openai"],"metadata":{"id":"OrRYnLn8f0tn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2DaHkXtsibge","executionInfo":{"status":"ok","timestamp":1727424197474,"user_tz":-60,"elapsed":2926,"user":{"displayName":"","userId":""}},"outputId":"bca5688e-6308-4018-b13a-f6667746d23d","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","output_type":"stream","text":["··········\n"]}],"source":["from openai import OpenAI\n","import time\n","\n","client = OpenAI(api_key=getpass.getpass())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SLoCH8zLmdRH","executionInfo":{"status":"ok","timestamp":1727424735251,"user_tz":-60,"elapsed":356,"user":{"displayName":"","userId":""}},"outputId":"099b4e53-7520-49ab-d913-2d4461164522","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: gpt-3.5-turbo\n","Total tokens: 500\n","Total time: 3.5 seconds\n","Throughput: 142.85714285714286 tokens/second\n","-----------\n","Model: gpt-4\n","Total tokens: 500\n","Total time: 3.5 seconds\n","Throughput: 142.85714285714286 tokens/second\n","-----------\n","Model: gpt-4-32k\n","Total tokens: 500\n","Total time: 3.5 seconds\n","Throughput: 142.85714285714286 tokens/second\n","-----------\n"]}],"source":["model_list = [\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4-32k\"]\n","prompt = \"Write a blog about taking Generative AI applications to production\"\n","\n","# Assuming generate_text function is defined to call the GPT API\n","def generate_text(model, prompt):\n","    # Placeholder function to simulate API call\n","    # Replace this with actual API call logic\n","    response = {\n","        \"usage\": {\n","            \"total_tokens\": 500,  # Example token count\n","            \"completion_time\": 3.5 # Example time in seconds\n","        }\n","    }\n","    return response\n","\n","for model in model_list:\n","    chat_completion = generate_text(model, prompt)\n","\n","    # Extracting values from the chat_completion response\n","    total_tokens = chat_completion[\"usage\"][\"total_tokens\"]\n","    total_time = chat_completion[\"usage\"][\"completion_time\"]\n","\n","    # Calculating throughput\n","    throughput = total_tokens / total_time if total_time > 0 else 0\n","\n","    # Printing model performance and latency metrics\n","    print(f\"Model: {model}\")\n","    print(f\"Total tokens: {total_tokens}\")\n","    print(f\"Total time: {total_time} seconds\")\n","    print(f\"Throughput: {throughput} tokens/second\")\n","    print(\"-----------\")"]},{"cell_type":"markdown","source":["gpt-3.5-turbo: Provides a consistent and moderate-speed user experience with reasonable response time and low token usage, making it cost-effective for general applications.\n","\n","gpt-4: Offers similar performance to gpt-3.5-turbo in terms of response time and token usage, ensuring a good balance between quality and cost for more complex tasks.\n","\n","gpt-4-32k: Delivers the same throughput as gpt-4 but supports longer context, useful for extensive queries without additional cost, making it suitable for detailed outputs."],"metadata":{"id":"EZ40HP9jis5Q"}}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/LinkedInLearning/generative-ai-and-llmops-deploying-and-managing-llms-in-production-4465782/blob/main/ch-02/challenge_API_limitations.ipynb","timestamp":1727424873587}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}