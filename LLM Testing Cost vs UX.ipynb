{"cells":[{"cell_type":"markdown","metadata":{"id":"Ibo6mCr3o0ZF"},"source":["# Testing API LLM API Tradeoffs (UX vs Cost)"]},{"cell_type":"markdown","metadata":{"id":"rovO3pgHo0ZH"},"source":["[![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LinkedInLearning/generative-ai-and-llmops-deploying-and-managing-llms-in-production-4465782/blob/solution/ch-02/challenge_API_limitations.ipynb)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"d0-Q5FXjfKHK","executionInfo":{"status":"ok","timestamp":1727426456249,"user_tz":-60,"elapsed":18720,"user":{"displayName":"","userId":""}}},"outputs":[],"source":["!pip install groq openai -q"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a9esvOP1ap8R","outputId":"73c3cb29-fa4f-43ef-e183-a93c3494fc48","executionInfo":{"status":"ok","timestamp":1727426435975,"user_tz":-60,"elapsed":9217,"user":{"displayName":"","userId":""}}},"outputs":[{"name":"stdout","output_type":"stream","text":["··········\n"]}],"source":["from groq import Groq\n","import getpass\n","\n","# Get token from console.groq.com\n","client = Groq(api_key=getpass.getpass())"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"aMXH-N19gW23","executionInfo":{"status":"ok","timestamp":1727426489328,"user_tz":-60,"elapsed":700,"user":{"displayName":"","userId":""}}},"outputs":[],"source":["def generate_text(model, prompt):\n","    chat_completion = client.chat.completions.create(\n","        model=model,\n","        messages=[\n","            {\"role\": \"user\", \"content\": prompt}\n","        ])\n","    return chat_completion"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nUGTqYWtgdI_","outputId":"5f338446-64f3-49cd-fba4-6e605d002d97","executionInfo":{"status":"ok","timestamp":1727426497405,"user_tz":-60,"elapsed":5753,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: llama3-8b-8192\n","Total tokens: 805\n","Total time: 0.653333333\n","Throughput: 1232.1428577715014\n","-----------\n","Model: llama3-70b-8192\n","Total tokens: 794\n","Total time: 2.478155744\n","Throughput: 320.3995559691506\n","-----------\n","Model: mixtral-8x7b-32768\n","Total tokens: 743\n","Total time: 1.177711155\n","Throughput: 630.8847435515715\n","-----------\n","Model: gemma-7b-it\n","Total tokens: 479\n","Total time: 0.627873152\n","Throughput: 762.8929481603316\n","-----------\n"]}],"source":["model_list=['llama3-8b-8192', 'llama3-70b-8192', 'mixtral-8x7b-32768', 'gemma-7b-it']\n","prompt=\"Write a blog about taking Generative AI applications to production\"\n","\n","for model in model_list:\n","    chat_completion = generate_text(model, prompt)\n","    total_tokens=chat_completion.usage.total_tokens\n","    total_time=chat_completion.usage.completion_time\n","    print(f\"Model: {model}\")\n","    print(f\"Total tokens: {total_tokens}\")\n","    print(f\"Total time: {total_time}\")\n","    print(f\"Throughput: {total_tokens/total_time}\")\n","    print(\"-----------\")"]},{"cell_type":"markdown","metadata":{"id":"2hLOlOqGka_U"},"source":["## OpenAI"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2DaHkXtsibge","outputId":"b5d6fa50-2409-48c7-9f86-274c4d3d6090","executionInfo":{"status":"ok","timestamp":1727426538252,"user_tz":-60,"elapsed":7268,"user":{"displayName":"","userId":""}}},"outputs":[{"name":"stdout","output_type":"stream","text":["··········\n"]}],"source":["from openai import OpenAI\n","import time\n","\n","client = OpenAI(api_key=getpass.getpass())"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZCYV1Dv2keKx","outputId":"835aa513-f499-47c6-9fc7-ccbd995a5351","executionInfo":{"status":"ok","timestamp":1727426735680,"user_tz":-60,"elapsed":62213,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: gpt-3.5-turbo\n","Total tokens: 437\n","Total time: 4.186880826950073\n","Throughput: 104.37364187371246\n","-----------\n","Model: gpt-4\n","Total tokens: 674\n","Total time: 34.953261852264404\n","Throughput: 19.282892762591647\n","-----------\n","Model: gpt-4o\n","Total tokens: 850\n","Total time: 61.952709436416626\n","Throughput: 13.720142472095961\n","-----------\n"]}],"source":["gpt_model_list = [\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4o\"]\n","prompt=\"Write a blog about taking Generative AI applications to production\"\n","start_time=time.time()\n","\n","for model in gpt_model_list:\n","    chat_completion = generate_text(model, prompt)\n","    total_tokens=chat_completion.usage.total_tokens\n","    total_time=time.time()-start_time\n","    print(f\"Model: {model}\")\n","    print(f\"Total tokens: {total_tokens}\")\n","    print(f\"Total time: {total_time}\")\n","    print(f\"Throughput: {total_tokens/total_time}\")\n","    print(\"-----------\")"]},{"cell_type":"markdown","source":["\n","## Model Summaries:\n","### Grok\n","* llama3-8b-8192: High-speed, cost-efficient model delivering excellent user experience with low latency, suitable for real-time, moderately complex tasks.\n","* llama3-70b-8192: Provides detailed and nuanced responses at the cost of increased latency and higher token usage, best for in-depth, non-real-time queries.\n","* mixtral-8x7b-32768: Balanced throughput and response time make it versatile for longer inputs, offering a good trade-off between speed and complexity.\n","* gemma-7b-it: Quick and cost-effective, ideal for fast interactions and less complex tasks, ensuring good UX with low latency and cost.\n","\n","### OpenAI\n","* gpt-3.5-turbo: Moderate speed and cost, but lower throughput compared to Grok models, making it less ideal for high-performance needs.\n","* gpt-4: High complexity but significantly slow with high cost, suitable only for tasks where detailed output is crucial and time/cost are less of a concern.\n","* gpt-4o: Slowest model with the highest cost, offering detailed responses but at a substantial expense, limiting its practical use for business applications.\n","## Overall Summary:\n","Grok models generally provide superior user experience with lower latency and cost efficiency, making them more viable for real-time applications and balanced business value. OpenAI models, particularly gpt-4 and gpt-4o, offer high complexity but are hindered by high costs and slow response times, reducing their business value for real-time applications. For most business use cases, Grok models offer a better balance between user experience and cost-effectiveness."],"metadata":{"id":"Dv4bqZUlqd2b"}}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/LinkedInLearning/generative-ai-and-llmops-deploying-and-managing-llms-in-production-4465782/blob/solution/ch-02/challenge_API_limitations.ipynb","timestamp":1727426944765}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}